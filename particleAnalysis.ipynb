{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import random\n",
    "import matplotlib.pylab as pl\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Intoduction\n",
    "\n",
    "The following analysis used a dataset containing 1000 muon+ and 1000 electrons created at 1 GeV. The detector consisted of 2 thin silicon tracking detectors used to determine the charge of the particles, 5 liquid Argon calorimeters (as models for an electromagnetic calorimeter) and a block of iron serving as a muon detector.\n",
    "\n",
    "The main assumption underlying the tracking procedure was that the particles were energetic enough to travel in a streight line. A 0.1 T magnetic field in the x direction was applied to produce tracking curvatures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Electron data\n",
    "eEnergyData = pd.read_csv( \"electronData/output_nt_Energy.csv\", comment=\"#\",\n",
    "                          names=[ \"Truth\", \"Particle\", \"Track1\", \"Track2\", \"Layer1\", \"Layer2\",\n",
    "                                \"Layer3\", \"Layer4\", \"Layer5\", \"Muon\"], index_col = False )\n",
    "ex1Data = pd.read_csv( \"electronData/output_nt_Tracker1_x.csv\", comment=\"#\",\n",
    "                          names=[ \"EventID\",\"x\"])\n",
    "ex2Data = pd.read_csv( \"electronData/output_nt_Tracker2_x.csv\", comment=\"#\",\n",
    "                           names=[ \"EventID\",\"x\"])\n",
    "ey1Data = pd.read_csv( \"electronData/output_nt_Tracker1_y.csv\", comment=\"#\",\n",
    "                           names=[ \"EventID\",\"y\"])\n",
    "ey2Data = pd.read_csv( \"electronData/output_nt_Tracker2_y.csv\", comment=\"#\",\n",
    "                           names=[ \"EventID\",\"y\"])\n",
    "\n",
    "#Proton data\n",
    "pEnergyData = pd.read_csv( \"protonData/output_nt_Energy.csv\", comment=\"#\",\n",
    "                          names=[ \"Truth\", \"Particle\", \"Track1\", \"Track2\", \"Layer1\", \"Layer2\",\n",
    "                                \"Layer3\", \"Layer4\", \"Layer5\", \"Muon\"], index_col = False )\n",
    "px1Data = pd.read_csv( \"protonData/output_nt_Tracker1_x.csv\", comment=\"#\",\n",
    "                          names=[ \"EventID\",\"x\"])\n",
    "px2Data = pd.read_csv( \"protonData/output_nt_Tracker2_x.csv\", comment=\"#\",\n",
    "                           names=[ \"EventID\",\"x\"])\n",
    "py1Data = pd.read_csv( \"protonData/output_nt_Tracker1_y.csv\", comment=\"#\",\n",
    "                           names=[ \"EventID\",\"y\"])\n",
    "py2Data = pd.read_csv( \"protonData/output_nt_Tracker2_y.csv\", comment=\"#\",\n",
    "                           names=[ \"EventID\",\"y\"])\n",
    "\n",
    "#Muon+ data\n",
    "mPlusEnergyData = pd.read_csv( \"mu+Data/output_nt_Energy.csv\", comment=\"#\",\n",
    "                          names=[ \"Truth\", \"Particle\", \"Track1\", \"Track2\", \"Layer1\", \"Layer2\",\n",
    "                                \"Layer3\", \"Layer4\", \"Layer5\", \"Muon\"], index_col = False )\n",
    "mPlusx1Data = pd.read_csv( \"mu+Data/output_nt_Tracker1_x.csv\", comment=\"#\",\n",
    "                          names=[ \"EventID\",\"x\"])\n",
    "mPlusx2Data = pd.read_csv( \"mu+Data/output_nt_Tracker2_x.csv\", comment=\"#\",\n",
    "                           names=[ \"EventID\",\"x\"])\n",
    "mPlusy1Data = pd.read_csv( \"mu+Data/output_nt_Tracker1_y.csv\", comment=\"#\",\n",
    "                           names=[ \"EventID\",\"y\"])\n",
    "mPlusy2Data = pd.read_csv( \"mu+Data/output_nt_Tracker2_y.csv\", comment=\"#\",\n",
    "                           names=[ \"EventID\",\"y\"])\n",
    "\n",
    "#Muon- data\n",
    "mMinEnergyData = pd.read_csv( \"mu-Data/output_nt_Energy.csv\", comment=\"#\",\n",
    "                          names=[ \"Truth\", \"Particle\", \"Track1\", \"Track2\", \"Layer1\", \"Layer2\",\n",
    "                                \"Layer3\", \"Layer4\", \"Layer5\", \"Muon\"], index_col = False )\n",
    "mMinx1Data = pd.read_csv( \"mu-Data/output_nt_Tracker1_x.csv\", comment=\"#\",\n",
    "                          names=[ \"EventID\",\"x\"])\n",
    "mMinx2Data = pd.read_csv( \"mu-Data/output_nt_Tracker2_x.csv\", comment=\"#\",\n",
    "                           names=[ \"EventID\",\"x\"])\n",
    "mMiny1Data = pd.read_csv( \"mu-Data/output_nt_Tracker1_y.csv\", comment=\"#\",\n",
    "                           names=[ \"EventID\",\"y\"])\n",
    "mMiny2Data = pd.read_csv( \"mu-Data/output_nt_Tracker2_y.csv\", comment=\"#\",\n",
    "                           names=[ \"EventID\",\"y\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eEnergy = np.asarray(eEnergyData )\n",
    "ex1 = np.asarray(ex1Data)\n",
    "ey1 = np.asarray(ey1Data)\n",
    "ex2 = np.asarray(ex2Data)\n",
    "ey2 = np.asarray(ey2Data)\n",
    "\n",
    "mPlusEnergy = np.asarray(mPlusEnergyData )\n",
    "mPlusx1 = np.asarray(mPlusx1Data)\n",
    "mPlusy1 = np.asarray(mPlusy1Data)\n",
    "mPlusx2 = np.asarray(mPlusx2Data)\n",
    "mPlusy2 = np.asarray(mPlusy2Data)\n",
    "\n",
    "mMinEnergy = np.asarray(mMinEnergyData )\n",
    "mMinx1 = np.asarray(mMinx1Data)\n",
    "mMiny1 = np.asarray(mMiny1Data)\n",
    "mMinx2 = np.asarray(mMinx2Data)\n",
    "mMiny2 = np.asarray(mMiny2Data)\n",
    "\n",
    "pEnergy = np.asarray(pEnergyData )\n",
    "px1 = np.asarray(px1Data)\n",
    "py1 = np.asarray(py1Data)\n",
    "px2 = np.asarray(px2Data)\n",
    "py2 = np.asarray(py2Data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reconstruction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get electron pairs.\n",
    "electronPairs = []\n",
    "\n",
    "for i in range (len(ex1)):\n",
    "    for j in range (len(ex2)):\n",
    "        if ex1[i,0] == ex2[j,0]:\n",
    "            if (math.fabs(ex1[i,1] - ex2[j,1]) < 0.5):\n",
    "                #Store as (x1, x2, y1, y2, EventID)\n",
    "                electronPairs.append([ex1[i,1], ex2[j,1], ey1[i,1], ey2[j,1], ex1[i,0]]) \n",
    "                \n",
    "#Get muon+ pairs.\n",
    "muonPlusPairs = []\n",
    "\n",
    "for i in range (len(mPlusx1)):\n",
    "    for j in range (len(mPlusx2)):\n",
    "        if mPlusx1[i,0] == mPlusx2[j,0]:\n",
    "            if (math.fabs(mPlusx1[i,1] - mPlusx2[j,1]) < 2.5):\n",
    "                #Store as (x1, x2, y1, y2, EventID)\n",
    "                muonPlusPairs.append([mPlusx1[i,1], mPlusx2[j,1], mPlusy1[i,1], mPlusy2[j,1], mPlusx1[i,0]]) \n",
    "                \n",
    "#Check using tracking.\n",
    "electrons = []\n",
    "muonPlus = []\n",
    "\n",
    "particlePairs = np.concatenate((electronPairs, muonPlusPairs))\n",
    "\n",
    "#Classify using tracking.\n",
    "for pair in particlePairs:\n",
    "    if pair[2] > pair[3]:\n",
    "        electrons.append(pair)\n",
    "    else:\n",
    "        muonPlus.append(pair)\n",
    "        \n",
    "#Match with corresponding energy signature.\n",
    "mPlusPairEnergy = []\n",
    "for i in range (len(mPlusEnergy)):\n",
    "    for pair in muonPlus:\n",
    "        if pair[4] == i:\n",
    "            mPlusPairEnergy.append(np.concatenate((mPlusEnergy[i], pair)))\n",
    "            \n",
    "ePairEnergy = []\n",
    "for i in range (len(eEnergy)):\n",
    "    for pair in electrons:\n",
    "        if pair[4] == i:\n",
    "            ePairEnergy.append(np.concatenate((eEnergy[i], pair)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eFrame = pd.DataFrame(data = ePairEnergy, index = None, columns = [ \"Truth\", \"Particle\", \"Track1\", \n",
    "                                                                  \"Track2\", \"Layer1\", \"Layer2\", \n",
    "                                                                  \"Layer3\", \"Layer4\", \"Layer5\", \n",
    "                                                                  \"Muon\", \"x1\", \"x2\", \"y1\", \"y2\", \"ID\"])\n",
    "\n",
    "muPlusFrame = pd.DataFrame(data = mPlusPairEnergy, index = None, columns = [ \"Truth\", \"Particle\", \"Track1\", \n",
    "                                                                  \"Track2\", \"Layer1\", \"Layer2\", \n",
    "                                                                  \"Layer3\", \"Layer4\", \"Layer5\", \n",
    "                                                                  \"Muon\", \"x1\", \"x2\", \"y1\", \"y2\", \"ID\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from tensorflow.python.keras.regularizers import l1_l2\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from tensorflow.python.keras.utils import plot_model\n",
    "from tensorflow.python.keras.models import Model, load_model\n",
    "from tensorflow.python.keras.layers import Input, Dense, BatchNormalization, Activation, Dropout\n",
    "from tensorflow.python.keras import backend as K\n",
    "from sklearn.model_selection import train_test_split\n",
    "from scikitplot.metrics import plot_roc\n",
    "from bayes_opt import BayesianOptimization\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calibration Plots\n",
    "eEnergySum = np.asarray([sum(Evalue[2:]) for Evalue in eEnergy])\n",
    "mPlusEnergySum = np.asarray([sum(Evalue[2:]) for Evalue in mPlusEnergy])\n",
    "\n",
    "eEnergyTruth = np.asarray(eEnergyData[\"Truth\"])\n",
    "mPlusEnergyTruth = np.asarray(mPlusEnergyData[\"Truth\"])\n",
    "\n",
    "eCalibMean = np.mean(eEnergyTruth/eEnergySum)\n",
    "eCalib = eCalibMean*eEnergySum\n",
    "\n",
    "mPlusCalibMean = np.mean(mPlusEnergyTruth/mPlusEnergySum)\n",
    "mPlusCalib = mPlusCalibMean*mPlusEnergySum\n",
    "\n",
    "eRatio = (eCalib - eEnergyTruth )/eEnergyTruth \n",
    "mPlusRatio = (mPlusCalib - mPlusEnergyTruth )/mPlusEnergyTruth \n",
    "\n",
    "n_bins = 20\n",
    "\n",
    "fig, axs = plt.subplots(1, 2, sharey=True)\n",
    "\n",
    "axs[0].hist(eRatio, bins=n_bins, range = (-0.2,0.2))\n",
    "axs[0].set_xlabel('Calibrated')\n",
    "axs[0].set_title('Electron')\n",
    "axs[1].hist(mPlusRatio, bins=n_bins, range = (-0.2,0.2))\n",
    "axs[1].set_xlabel('Calibrated')\n",
    "axs[1].set_title('Muon+');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Resolution:\\nElectron = {:.4f}\\nMuon+ = {:.4f}\".format(np.std(eRatio), np.std(mPlusRatio)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clasification model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eTrain = np.asarray(eFrame[[\"Layer1\", \"Layer2\",\"Layer3\", \"Layer4\", \"Layer5\", \"Muon\", \"y1\", \"y2\"]])\n",
    "eTarget = np.asarray(eFrame[\"Particle\"]) - 1\n",
    "muPlusTrain = np.asarray(muPlusFrame[[\"Layer1\", \"Layer2\",\"Layer3\", \"Layer4\", \"Layer5\", \"Muon\", \"y1\", \"y2\"]])\n",
    "muPlusTarget = np.asarray(muPlusFrame[\"Particle\"]) - 1\n",
    "\n",
    "target = np.concatenate((eTarget[:1300], muPlusTarget[:1300]))\n",
    "train = np.concatenate((eTrain[:1300], muPlusTrain[:1300]))\n",
    "\n",
    "#Shuffle datasets\n",
    "perm_train = np.random.permutation(train.shape[0])\n",
    "\n",
    "train = train[perm_train]\n",
    "target = target[perm_train]\n",
    "\n",
    "#Standardise data.\n",
    "train = StandardScaler().fit_transform(train)\n",
    "\n",
    "#Split into train/test sets.\n",
    "x_train, x_test, y_train, y_test = train_test_split(train, target, test_size = 0.3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define testing model.\n",
    "\n",
    "def get_model(nb_layer1, nb_layer2, nb_layer3, dropout1, dropout2):\n",
    "    \n",
    "    opts_dense = dict(activation = 'relu')\n",
    "    \n",
    "    i = Input(shape = 8, name = 'Input')\n",
    "    x = Dense(nb_layer1, **opts_dense)(i)\n",
    "    x = Dropout(dropout1)(x)\n",
    "    x = Dense(nb_layer2, **opts_dense)(x)\n",
    "    x = Dropout(dropout2)(x)\n",
    "    x = Dense(nb_layer3, **opts_dense)(x)\n",
    "    o = Dense(2, activation = 'softmax', name = 'Output')(x)\n",
    "    \n",
    "    return Model(i, o)\n",
    "\n",
    "def fit_with(nb_layer1, nb_layer2, nb_layer3, dropout1, dropout2, lr):\n",
    "     \n",
    "    model = get_model(nb_layer1, nb_layer2, nb_layer3, dropout1, dropout2)\n",
    "    \n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate = lr)\n",
    "    \n",
    "    model.compile(optimizer = optimizer, \n",
    "            metrics = ['accuracy'], \n",
    "            loss = 'sparse_categorical_crossentropy')\n",
    "    \n",
    "    model.fit(x_train, y_train, epochs = 5, \n",
    "              verbose = 0, batch_size = 64, validation_data = [x_test, y_test], shuffle = True);\n",
    "    \n",
    "    loss, acc = model.evaluate(x_test, y_test)\n",
    "    print('Loss: {:.4f}, Accuracy: {:.4f}'.format(loss, acc*100))\n",
    "    \n",
    "    return acc\n",
    "\n",
    "\n",
    "#Peform optimisation.\n",
    "\n",
    "pbounds = {'nb_layer1': (10,100), 'nb_layer2': (10,100), 'nb_layer3': (10,100), \n",
    "           'dropout1': (0.1,0.5), 'dropout2': (0.1,0.5), 'lr': (1e-4, 1e-2)}\n",
    "\n",
    "optimizer = BayesianOptimization(f = fit_with, pbounds = pbounds, verbose = 2,  random_state = 1)\n",
    "\n",
    "optimizer.maximize(init_points= 3, n_iter= 2)\n",
    "\n",
    "for i, res in enumerate(optimizer.res):\n",
    "    print(\"Iteration {}: \\n\\t{}\".format(i, res))\n",
    "\n",
    "print(optimizer.max)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def DNN_Classifier(nb_feats, nb_target):\n",
    "    \n",
    "    opts_dense = dict(activation = 'relu')\n",
    "    \n",
    "    i = Input(shape = nb_feats, name = 'Input')\n",
    "    x = Dense(59, **opts_dense)(i)\n",
    "    x = Dropout(0.18)(x)               \n",
    "    x = Dense(48, **opts_dense)(x)\n",
    "    x = Dropout(0.24)(x)\n",
    "    x = Dense(71, **opts_dense)(x)\n",
    "    o = Dense(nb_target, activation = 'softmax', name = 'Output')(x)\n",
    "    \n",
    "    return Model(i, o, name = 'DNN_Classifier')\n",
    "\n",
    "#Construct and compile model.\n",
    "\n",
    "dnn = DNN_Classifier(x_train.shape[1], 3)\n",
    "dnn.compile(optimizer = tf.keras.optimizers.Adam(learning_rate = 0.004), \n",
    "            metrics = ['accuracy'], \n",
    "            loss = 'sparse_categorical_crossentropy')\n",
    "\n",
    "#Train model.\n",
    "dnn.fit(x_train, y_train, epochs = 50, verbose = 1,\n",
    "        batch_size = 64, validation_data = [x_test, y_test], shuffle = True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot loss and save model.\n",
    "dnn.save('dnn.h5')\n",
    "\n",
    "loss = dnn.history.history['loss']\n",
    "val_loss = dnn.history.history['val_loss']\n",
    "epochs = list(range(len(dnn.history.history['loss'])))\n",
    "\n",
    "plt.plot(epochs, loss, label = 'Loss')\n",
    "plt.plot(epochs, val_loss, label = 'Val loss')\n",
    "plt.legend()\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Evaluate model accuracy.\n",
    "loss, acc = dnn.evaluate(x_test, y_test)\n",
    "print('Model accuracy = {:.2f}%'.format(acc*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test over energy range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eEnergyRange = pd.read_csv( \"electron(500-10500)/output_nt_Energy.csv\", comment=\"#\",\n",
    "                          names=[ \"Truth\", \"Particle\", \"Track1\", \"Track2\", \"Layer1\", \"Layer2\",\n",
    "                                \"Layer3\", \"Layer4\", \"Layer5\", \"Muon\"], index_col = False )\n",
    "\n",
    "eEnergyArray = np.asarray(eEnergyRange)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eEnergySum = np.asarray([sum(Evalue[2:]) for Evalue in eEnergyArray])\n",
    "eEnergyTruth = np.asarray(eEnergyRange[\"Truth\"])\n",
    "\n",
    "eCalibMean = np.mean(eEnergyTruth/eEnergySum)\n",
    "eCalib = eCalibMean*eEnergySum\n",
    "eRatio = (eCalib - eEnergyTruth )/eEnergyTruth \n",
    "\n",
    "pl.hist2d(eEnergyTruth, eRatio, bins = (10, 20))\n",
    "pl.xlabel('True Energy [MeV]')\n",
    "pl.ylabel('(Calibrated E − True E) / True E')\n",
    "pl.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Improvements discussion:\n",
    "\n",
    "Preliminary runs failed to detect the muons with only 56% of the total energy deposited. An iron muon detector was added into the simulation in order to examine the muons more accuratelly. This allowed for a clear detection of muons up to 10GeV after which the particle passed through the detector. \n",
    "\n",
    "The machine learning model used in classification identified the 2 particle types (electrons and muons+) with 99.87% accuracy. This is unsurprising since the particles had opposite charges and therefore demonstrated curvature in opposite directions (determined by the y-coordinate). Additionally, muons deposited a significantly larger fraction of their energy in the iron muon detector compared to the electron which showered in the liquid Argon calorimeters depositing its entire energy. Therefore, the likely high lever features used to classify the particles were their spacial coordinates (specifically the y-coordinate) and energy signatures.\n",
    "\n",
    "The thin silicon tracking detectors spacing was varied to allow for a clear curviture even at higher energies.\n",
    "Various detector stuctures were tested (included in the Failed folder), however the one used to gather the data was chosen for its versitility to detect a range of particle types including muon+, muon-, electron and proton."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
